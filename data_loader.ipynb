import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import pickle

class YelpDataset(Dataset):
    def __init__(self, data_path, embedding_path):
        # Load data from data_path
        self.data = self.load_data(data_path)
        # Load word embeddings
        self.word_embeddings = self.load_word_embeddings(embedding_path)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        review_text, rating = self.data[idx]
        # Convert review text to word embeddings
        review_embedding = self.get_review_embedding(review_text)
        return review_embedding, rating

    def load_data(self, data_path):
        # Load data from CSV file
        df = pd.read_csv(data_path)
        # Preprocess the data if necessary
        # For simplicity, we'll assume the CSV file has 'review_text' and 'rating' columns
        reviews = df['review_text'].tolist()
        ratings = df['rating'].tolist()
        return list(zip(reviews, ratings))

    def load_word_embeddings(self, embedding_path):
        # Load word embeddings from pickle file
        with open(embedding_path, 'rb') as f:
            word_embeddings = pickle.load(f)
        return word_embeddings

   def get_review_embedding(self, review_text):
    # Tokenize the review text
    tokens = review_text.split()
    # Initialize an empty list to store the embeddings for each token
    embeddings = []
    # Iterate over tokens and look up embeddings
    for token in tokens:
        # Check if the token exists in the word embeddings dictionary
        if token in self.word_embeddings:
            # If the token exists, append its embedding to the list
            embeddings.append(self.word_embeddings[token])
        else:
            # If the token is not found in the embeddings, you may handle this case as needed
            # For simplicity, we'll initialize a random embedding for unknown tokens
            # Alternatively, you could skip unknown tokens or use a special <UNK> token embedding
            # Here, we assume word embeddings is a dictionary where keys are words and values are embeddings
            # You should replace this with your actual word embeddings data structure
            random_embedding = torch.randn(EMBEDDING_DIM)  # Replace EMBEDDING_DIM with your embedding dimension
            embeddings.append(random_embedding)
    # Convert the list of embeddings to a tensor
    review_embedding = torch.stack(embeddings)
    return review_embedding


def get_data_loader(data_path, embedding_path, batch_size, shuffle=True):
    dataset = YelpDataset(data_path, embedding_path)
    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)
    return data_loader
